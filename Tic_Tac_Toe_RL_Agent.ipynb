{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83a8b086",
   "metadata": {},
   "source": [
    "\n",
    "# Tic Tac Toe Reinforcement Learning Agent\n",
    "\n",
    "Dieses Notebook implementiert einen Reinforcement Learning (RL)-Algorithmus, der Q-Learning verwendet, damit der Computer lernt, wie man Tic Tac Toe spielt.\n",
    "\n",
    "Das Spielbrett ist fÃ¼r ein 3x3-Gitter ausgelegt, und der Computer lernt, durch \"Exploration\" und \"Exploitation\" optimale ZÃ¼ge zu machen und seine Strategie auf der Grundlage des Q-Learning-Ansatzes zu verbessern.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39614b68",
   "metadata": {},
   "source": [
    "Ihre Umgebung sollte mithilfe der Informationen aus dem README.md vorbereitet sein und Sie kÃ¶nnen nun mit dem Notebook fortfahren."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42857966",
   "metadata": {},
   "source": [
    "\n",
    "### Step 1: Spielumgebung \n",
    "\n",
    "In diesem Abschnitt instanzieren wir ein Game Objekt vom Typ \"Game\", welches im Game.py Dokument als Klasse definiert wurde.\n",
    "\n",
    "Die Methoden in der Klasse sollten Ihnen bereits bekannt vorkommen. Es sind die selben wie beim Tic Tac Toe Spiel, welches Sie bereits kennengelernt haben.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc535dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Game import Game\n",
    "\n",
    "myGame = Game()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8751876",
   "metadata": {},
   "source": [
    "\n",
    "### Step 2: Q-Learning Algorithm\n",
    "\n",
    "In diesem Abschnitt instanzieren wir den Q-Learning-Agenten. Die Klasse des Agenten wird im QLearningAgent.py Dokument definiert.\n",
    "Die Klasse enthÃ¤lt Methoden, die es dem Agenten erlauben mit dem Spiel zu spielen. Ausserdem besitzt die Klasse eine Methode train_agent() damit der Agent aus Erfahrungen lernen kann. Wir verwenden eine Q-Tabelle, um die Werte der Aktionen fÃ¼r jeden Spielzustand zu speichern.\n",
    "\n",
    "- myGame: Die Umgebung oder das Spiel, mit dem der Agent interagiert.\n",
    "- 0.5: Die Lernrate (Alpha) - Bestimmt, wie stark neue Informationen die gelernten Werte beeinflussen.\n",
    "- Ein Wert von 0,5 bedeutet, dass der Agent neue Informationen und frÃ¼here Erfahrungen gleich gewichtet.\n",
    "- 0.9: Der Diskontierungsfaktor (Gamma) - Bestimmt die Bedeutung zukÃ¼nftiger Belohnungen im Vergleich zu sofortigen Belohnungen.\n",
    "- Ein Wert nahe 1,0 bedeutet, dass der Agent langfristigen Belohnungen den Vorzug gibt.\n",
    "- 0.1: Die Erkundungsrate (epsilon) - Die Wahrscheinlichkeit, dass der Agent erkundet (eine zufÃ¤llige Aktion wÃ¤hlt)\n",
    "- statt bekannte Informationen zu nutzen. Ein niedriger Wert begÃ¼nstigt die Ausbeutung gegenÃ¼ber der Erkundung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cf857e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from QLearningAgent import QLearningAgent\n",
    "\n",
    "agent = QLearningAgent(myGame, 0.5, 0.9, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1dd244",
   "metadata": {},
   "source": [
    "\n",
    "### Step 3: Training des Agenten\n",
    "\n",
    "In diesem Abschnitt trainieren wir den Agenten, indem wir ihn mehrere Episoden lang gegen einen Gegner spielen lassen, welcher zufÃ¤llige ZÃ¼ge ausfÃ¼hrt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d808df28",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 100000\n",
    "bot_wins, opponent_wins, draws = agent.train_agent(EPISODES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99dce58",
   "metadata": {},
   "source": [
    "\n",
    "### Step 4: Inspizieren des trainierten RL-Agenten\n",
    "\n",
    "Nachdem der Reinforcement-Learning-Agent trainiert wurde, ist es entscheidend, sein Verhalten zu analysieren und zu verstehen, was er tatsÃ¤chlich gelernt hat. Nur weil der Agent im Training gute Ergebnisse erzielt hat, bedeutet das nicht automatisch, dass er auch robuste oder nachvollziehbare Strategien entwickelt hat. Durch eine gezielte Inspektion kÃ¶nnen wir Ã¼berprÃ¼fen, ob das Lernverhalten sinnvoll ist, ob sich Muster erkennen lassen und ob es mÃ¶glicherweise noch unerwartete SchwÃ¤chen gibt. Diese Analyse ist ein wichtiger Schritt, um die QualitÃ¤t und Korrektheit des Trainingsprozesses zu validieren.\n",
    "\n",
    "ZunÃ¤chst Ã¼berprÃ¼fen wir die Anzahl der nicht-nullen Q-Werte in der Q-Tabelle im Vergleich zur Gesamtzahl aller mÃ¶glichen EintrÃ¤ge. Dies gibt uns einen Eindruck davon, wie viele ZustÃ¤nde Ã¼berhaupt wÃ¤hrend des Trainings relevant wurden und fÃ¼r welche der Agent tatsÃ¤chlich Entscheidungswerte gelernt hat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1563ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "q_table = defaultdict(lambda: np.zeros(9), agent.q_table)\n",
    "\n",
    "non_zero_entries = sum(np.any(values) for values in q_table.values())\n",
    "non_zero_entries, len(q_table)  # Nicht-Null-EintrÃ¤ge gegenÃ¼ber GesamteintrÃ¤gen in der Q-Tabelle\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898071bb",
   "metadata": {},
   "source": [
    "\n",
    "Anschliessend visualisieren wir die Lernstatistik in Form eines Diagramms. Hierbei wird die Siegquote Ã¼ber den Trainingsverlauf hinweg dargestellt, um die Lernkurve des Agenten nachvollziehen zu kÃ¶nnen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48490cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Die Lernstatistik als Grafen darstellen\n",
    "plt.plot(range(EPISODES), bot_wins, label=\"Bot gewinnt\", color=\"blue\")\n",
    "plt.plot(range(EPISODES), opponent_wins, label=\"Gegner gewinnt\", color=\"red\")\n",
    "plt.plot(range(EPISODES), draws, label=\"Unentschieden\", color=\"green\")\n",
    "plt.xlabel(\"Episoden\")\n",
    "plt.ylabel(\"Anz. Siege/Unentschieden\")\n",
    "plt.title(\"Spielstatistik wÃ¤hrend der Lernphase\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c58021",
   "metadata": {},
   "source": [
    "\n",
    "FÃ¼r eine detaillierte Betrachtung geben wir ausserdem fÃ¼r alle ZustÃ¤nde, in denen der Agent sinnvolle Werte gelernt hat (z.â€¯B. wenn hÃ¶chstens XX (kann selber definiert werden) der 9 mÃ¶glichen Aktionen einen Q-Wert von 0 haben), die Q-Werte als 3Ã—3-Matrix aus. Dies ermÃ¶glicht eine intuitive Interpretation, welche Aktionen in welcher Spielsituation bevorzugt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e68978",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "min_empty_fields = 6\n",
    "\n",
    "for idx, values in q_table.items():\n",
    "    # es wird fÃ¼r jeden Key (Spielzustand) gezÃ¤hlt, wie viele freie Felder im Value (alle Q-Werte) enthalten sind\n",
    "    space_count = sum(row.count(' ') for row in idx)\n",
    "    if space_count >= min_empty_fields and np.any(values < 0):  # Bedingung um herauszufiltern, wo mindestens XX Felder im State noch leer sind\n",
    "        # Spielzustand als 3x3-Matrix ausgeben\n",
    "        print(\"Board State:\")\n",
    "        for row in idx:\n",
    "            print(row)\n",
    "\n",
    "        # Q-Werte als 3x3-Matrix ausgeben\n",
    "        print(\"\\nQ-Values:\")\n",
    "        for i in range(0, 9, 3):\n",
    "            print(values[i:i + 3])\n",
    "\n",
    "        print(\"\\n\" + \"-\" * 20 + \"\\n\")  # Trennlinie zur besseren Lesbarkeit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17630f3",
   "metadata": {},
   "source": [
    "\n",
    "Zum Schluss erstellen wir fÃ¼r ausgewÃ¤hlte konkrete ZustÃ¤nde sogenannte Heatmaps. Diese zeigen visuell auf einen Blick, wie stark der Agent jede mÃ¶gliche Aktion in einem bestimmten Zustand bewertet. Sie helfen dabei, das Entscheidungsverhalten des Agenten besser zu verstehen und mit strategisch sinnvollen SpielzÃ¼gen im TicTacToe zu vergleichen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbef99d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def visualize_q_values(state):\n",
    "    \"\"\"Visualizes Q-values for a specific Tic Tac Toe game state.\"\"\"\n",
    "    state = tuple(tuple(row) for row in state)  # State in ein Tupel von Strings Ã¼bersetzen: entspricht dem key im Q-table\n",
    "    q_values = q_table.get(state, np.zeros(9))  # Q-Werte fÃ¼r den State abrufen, falls key nicht existiert alle Q-Werte mit zero besetzen\n",
    "    \n",
    "    # Shape der Q-Werte an ein 3x3 board layout anpassen\n",
    "    q_values_board = q_values.reshape((3, 3))\n",
    "\n",
    "    # Heatmap plotten\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(q_values_board, annot=True, cmap=\"coolwarm\", cbar=True, square=True)\n",
    "    plt.title(\"Q-values Heatmap for Selected Game State\")\n",
    "    plt.xlabel(\"Column\")\n",
    "    plt.ylabel(\"Row\")\n",
    "    plt.show()\n",
    "\n",
    "# Beispielhafter Zustand zur Veranschaulichung\n",
    "example_state = [['X', ' ', 'X'], \n",
    "                ['O', 'O', ' '], \n",
    "                [' ', 'X', ' ']]\n",
    "visualize_q_values(example_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebdef90",
   "metadata": {},
   "source": [
    "\n",
    "### Step 5: Exportieren des trainierten RL-Agenten\n",
    "\n",
    "Nach dem erfolgreichen Training mÃ¶chten wir den gelernten Wissensstand des RL-Agenten sichern, um ihn spÃ¤ter wiederverwenden oder analysieren zu kÃ¶nnen â€“ ohne erneut trainieren zu mÃ¼ssen. \n",
    "\n",
    "Zu diesem Zweck speichern wir die Q-Tabelle, die die erlernten Zustands-Aktions-Werte enthÃ¤lt, in einer externen Datei. Dies ermÃ¶glicht es uns, den Agenten zu einem spÃ¤teren Zeitpunkt zu laden und direkt in einem Spiel oder in einer Evaluierungsumgebung einzusetzen.\n",
    "\n",
    "Typischerweise erfolgt der Export im Format als Pickle-Datei (`.pkl`): Damit kann die Q-Tabelle im ursprÃ¼nglichen Python-Format gespeichert und wieder eingelesen werden.\n",
    "\n",
    "Vorteile des Exports:\n",
    "- Keine erneute Trainingszeit notwendig\n",
    "- ErmÃ¶glicht Analyse, Visualisierung oder Vergleich mehrerer TrainingslÃ¤ufe\n",
    "- Grundlage fÃ¼r den Einsatz des Agenten im \"echten Spielbetrieb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc339603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"trained_q_table.pkl\", \"wb\") as f:\n",
    "    pickle.dump(dict(agent.q_table), f) \n",
    "print(\"Model saved as 'trained_q_table.pkl'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc28cab",
   "metadata": {},
   "source": [
    "ðŸ” Wurde die Exportdatei erfolgreich erstellt?\n",
    "\n",
    "Bitte Ã¼berprÃ¼fe, ob im Projektverzeichnis eine neue Datei (z.â€¯B. trained_q_table.pkl) erstellt wurde. Wenn die Datei vorhanden ist, war der Export erfolgreich und der trainierte Agent kann spÃ¤ter wieder geladen werden.\n",
    "\n",
    "In der nÃ¤chsten Zelle wird noch gezeigt, wie man die Werte wieder laden und einem Agenten zuweisen kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b95e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Die Pickle-Datei wird eingelesen\n",
    "with open(\"trained_q_table.pkl\", \"rb\") as f:\n",
    "    q_table_data = pickle.load(f)\n",
    "    # Sicherstellen, dass q_table nach dem Laden vom Typ defaultdict ist\n",
    "    agent.q_table = defaultdict(lambda: np.zeros(9), q_table_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
